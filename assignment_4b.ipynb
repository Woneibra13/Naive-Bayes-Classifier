{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce2e03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tianw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e50afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "            \n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('neg', vocab)\n",
    "process_docs('pos', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c166f82",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c2de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# Updated process_docs for pos and neg lines of words\n",
    "def process_docs(directory, vocab):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines\n",
    "\n",
    "\n",
    "negative_lines = process_docs('neg', vocab)\n",
    "positive_lines = process_docs('pos', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec27f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260\n",
      "540\n",
      "1260\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Ref: https://github.com/Vakhshoori101/TwitterSentimentAnalysis/blob/main/Twitter%20Classification%20Template.ipynb\n",
    "# 70% Training: 630 out of 900, 30% Testing: 270 out of 900\n",
    "test_pos = positive_lines[630:]\n",
    "train_pos = positive_lines[:630]\n",
    "test_neg = negative_lines[630:]\n",
    "train_neg = negative_lines[:630]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f97b6",
   "metadata": {},
   "source": [
    "## Produce as output the top 5 positive and top 5 negative evidences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a73f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for the frequency of words appears in number of positive reviews\n",
    "pos_freq_list = {}\n",
    "# create a dictionary for the frequency of words appears in number of negative reviews\n",
    "neg_freq_list = {}\n",
    "\n",
    "#set the default appreance for each word to be 15 to prevent special case 0\n",
    "for token in vocab:\n",
    "    pos_freq_list[token] = 4\n",
    "    neg_freq_list[token] = 4\n",
    "    \n",
    "#This function will tokenize each review in the directory and add one to the counter if it appears in the review\n",
    "def count_freq(directory, freq_list):\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        doc = load_doc(path)\n",
    "        temp_token = Counter()\n",
    "        tok = clean_doc(doc)\n",
    "        temp_token.update(tok)\n",
    "        for w in temp_token:\n",
    "            freq_list[w] += 1\n",
    "    return freq_list\n",
    "\n",
    "#update the list for both pos and neg frequency\n",
    "neg_freq_list = count_freq('neg', neg_freq_list)\n",
    "pos_freq_list = count_freq('pos', pos_freq_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f3c03b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "  \n",
    "#Default number for totoal review is 1800, positive review is 900 and negative review also 900\n",
    "N = 1800\n",
    "pos = 900\n",
    "neg = 900\n",
    "\n",
    "neg_I = {}\n",
    "pos_I = {}\n",
    "\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "\n",
    "# Mutual information for all possible tokens in negative reviews\n",
    "for token in tokens:\n",
    "    neg_I[token] = log2((neg_freq_list[token]*N)/((neg_freq_list[token]+pos_freq_list[token])*neg))\n",
    "    \n",
    "    \n",
    "# Mutual information for all possible tokens in positive reviews\n",
    "for token in tokens:\n",
    "    pos_I[token] = log2((pos_freq_list[token]*N)/((neg_freq_list[token]+pos_freq_list[token])*pos))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a3b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: Word: outstanding\n",
      "P2: Word: religion\n",
      "P3: Word: wonderfully\n",
      "P4: Word: offbeat\n",
      "P5: Word: finest\n",
      "N1: Word: ludicrous\n",
      "N2: Word: idiotic\n",
      "N3: Word: wasted\n",
      "N4: Word: stupidity\n",
      "N5: Word: chuckle\n"
     ]
    }
   ],
   "source": [
    "# sorted the positive mutal information and get the top 5\n",
    "top_pos = sorted(pos_I.items(), key=lambda x:-x[1])[:5]\n",
    "# sorted the negative mutal information and get the top 5\n",
    "top_neg = sorted(neg_I.items(), key=lambda x:-x[1])[:5]\n",
    "for x in range(5):\n",
    "    print('P' + str(x+1) +': Word: ' + top_pos[x][0])\n",
    "    \n",
    "for x in range(5):\n",
    "    print('N' + str(x+1) +': Word: ' + top_neg[x][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
